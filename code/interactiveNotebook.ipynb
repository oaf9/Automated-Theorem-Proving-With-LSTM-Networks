{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proving Theorems In Propositional Logic with LSTM-Based Text Generators\n",
    "#### Author: Omar Afifi\n",
    "\n",
    "Consider a number of propositional (i.e. variable-free) sentences (premsises). For example: \n",
    "\n",
    "1. p→q \n",
    "2. o\n",
    "3. pv¬o\n",
    "\n",
    "A propositional proof from the premises to a conclusion (another sentence) is a sequence of variable-free statements that follow from the premises by logical deduction rules (e.g. modus ponens, modus tollens, modus tollendo ponens, dysjuntive elimination, etc ... )\n",
    "\n",
    "For example, a proof of the propositional sentence (q) from the preceding premises is as follows: \n",
    "\n",
    "4. ¬¬o (from 2, double negation)\n",
    "5. p (from 3 and 4, dysjuntive elimination )\n",
    "5. q (from 1 and 5, modus ponens)\n",
    "\n",
    "QED.\n",
    "\n",
    "\n",
    "This notebook explores the utility of using LSTM text-generators to generate a propositional proof given a collection of propositional sentences. Our hope is that it can be helpful as a stepping stone to making progress in the arena of stochastic theorem provers. \n",
    "\n",
    "Credits: Hugging Face User ergotts for building this dataset: https://huggingface.co/datasets/ergotts/propositional-logic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data and preparring the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/omarafifi/Downloads/LSTM-Prover/Automated-Theorem-Proving-With-LSTM-Networks/ls_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import process_data\n",
    "\n",
    "#load the data from hugging face mode = 'w' means that we are tokenizing words rather than characters or sentences. \n",
    "proofs_dataset = process_data.LoadLogicData(mode = 'w') \n",
    "\n",
    "#format the proofs: essentially just mapping words to integers and then creating n-gram sequences\n",
    "word_to_int, int_to_word, sequenced_proofs = process_data.generate_sequences(proofs_dataset)\n",
    "\n",
    "#split data into input and label by setting label equal to next word.\n",
    "#sequence length is the length of eqch sequence, this allows us to pack them during training. \n",
    "X, sequence_lengths,y = process_data.makeXy(sequenced_proofs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### making the data compatible with torch API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.utils.data  as data\n",
    "\n",
    "X = t.tensor(X, dtype = t.int64)\n",
    "sequence_lengths =  t.tensor(sequence_lengths, dtype = t.int64)\n",
    "y = t.tensor(y, dtype = t.int64).view(-1,1)\n",
    "\n",
    "torch_data = data.DataLoader(data.TensorDataset(X,sequence_lengths, y),\n",
    "                            batch_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50472, 106])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[63],\n",
       "        [14],\n",
       "        [52],\n",
       "        ...,\n",
       "        [57],\n",
       "        [13],\n",
       "        [ 1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Model and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    #constructor that inherits from nn.Module\n",
    "    def __init__(self, seq_length,  hidden_size, num_layers,vocab_size):\n",
    "\n",
    "        super(LSTM, self).__init__()\n",
    "        #should probably initilize the hidden states\n",
    "        self.seq_length = seq_length\n",
    "        self.hidden_dim = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        #we need to embed the words, a rule of thumb is that the \n",
    "        # embedding has the fourth root of the size of the vocabulary\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "\n",
    "        #initilize an lstm layer\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first = True)\n",
    "        \n",
    "        #output hidden layer\n",
    "        self.fc = nn.Linear(hidden_size, self.vocab_size)\n",
    "\n",
    "    def forward(self, X, sequence_lengths):\n",
    "        \"\"\" forward pass through network\"\"\"\n",
    "\n",
    "        X = self.embedding(X)\n",
    "\n",
    "        X = pack_padded_sequence(X, sequence_lengths, \n",
    "                                 batch_first = True, \n",
    "                                 enforce_sorted = False)\n",
    "\n",
    "\n",
    "        X, (H,C) = self.lstm(X)\n",
    "        X, _ = pad_packed_sequence(X, batch_first = True)\n",
    "\n",
    "        fc_out = self.fc(X)\n",
    "\n",
    "        return F.log_softmax(fc_out, dim = -1)\n",
    "\n",
    "\n",
    "    def train(self, train_loader, epochs, \n",
    "              loss_function, optimizer):\n",
    "\n",
    "        for epoch in range(epochs): # for each epoch\n",
    "\n",
    "            epoch_loss = 0\n",
    "            correct_count = 0\n",
    "            prediction_count = 0\n",
    "\n",
    "            for index, data in enumerate(train_loader): #one pass over the training data\n",
    "\n",
    "                X, sequence_lengths, y = data\n",
    "\n",
    "                optimizer.zero_grad() # zero gradients to avoid blowup\n",
    "                output = self.forward(X, sequence_lengths)\n",
    "\n",
    "                output = output[range(len(X)), sequence_lengths-1]\n",
    "\n",
    "                output = output.view(-1, self.vocab_size)\n",
    "\n",
    "                #print(output.shape)\n",
    "                y = y.view(-1)\n",
    "\n",
    "                loss = loss_function(output, y)\n",
    "\n",
    "                loss.backward()\n",
    "                #gradient clipping helps avoid blowup, which was a problem with training\n",
    "                torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=4)  \n",
    "                optimizer.step()\n",
    "   \n",
    "                #update metrics\n",
    "                epoch_loss += loss.item()\n",
    "                _, y_hat = torch.max(output, dim = 1)\n",
    "                correct_count += (y_hat == y).sum().item()\n",
    "                prediction_count += y.size(0)\n",
    "\n",
    "\n",
    "            #print metrics\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Accuracy: {correct_count/prediction_count:.4f}')\n",
    "            print('   ')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab_size = len(word_to_int)\n",
    "hidden_size = 6\n",
    "num_layers = 2\n",
    "loss_function = torch.nn.CrossEntropyLoss(reduction = \"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 3.0755\n",
      "Epoch [1/100], Accuracy: 0.0793\n",
      "   \n",
      "Epoch [2/100], Loss: 2.7707\n",
      "Epoch [2/100], Accuracy: 0.1847\n",
      "   \n",
      "Epoch [3/100], Loss: 2.6553\n",
      "Epoch [3/100], Accuracy: 0.2484\n",
      "   \n",
      "Epoch [4/100], Loss: 2.5764\n",
      "Epoch [4/100], Accuracy: 0.2663\n",
      "   \n",
      "Epoch [5/100], Loss: 2.5018\n",
      "Epoch [5/100], Accuracy: 0.2715\n",
      "   \n",
      "Epoch [6/100], Loss: 2.3714\n",
      "Epoch [6/100], Accuracy: 0.3112\n",
      "   \n",
      "Epoch [7/100], Loss: 2.2223\n",
      "Epoch [7/100], Accuracy: 0.3421\n",
      "   \n",
      "Epoch [8/100], Loss: 2.0735\n",
      "Epoch [8/100], Accuracy: 0.3480\n",
      "   \n",
      "Epoch [9/100], Loss: 1.9747\n",
      "Epoch [9/100], Accuracy: 0.3574\n",
      "   \n",
      "Epoch [10/100], Loss: 1.9227\n",
      "Epoch [10/100], Accuracy: 0.3686\n",
      "   \n",
      "Epoch [11/100], Loss: 1.8898\n",
      "Epoch [11/100], Accuracy: 0.3733\n",
      "   \n",
      "Epoch [12/100], Loss: 1.8638\n",
      "Epoch [12/100], Accuracy: 0.3783\n",
      "   \n",
      "Epoch [13/100], Loss: 1.8406\n",
      "Epoch [13/100], Accuracy: 0.3870\n",
      "   \n",
      "Epoch [14/100], Loss: 1.8178\n",
      "Epoch [14/100], Accuracy: 0.3964\n",
      "   \n",
      "Epoch [15/100], Loss: 1.7944\n",
      "Epoch [15/100], Accuracy: 0.4063\n",
      "   \n",
      "Epoch [16/100], Loss: 1.7706\n",
      "Epoch [16/100], Accuracy: 0.4132\n",
      "   \n",
      "Epoch [17/100], Loss: 1.7475\n",
      "Epoch [17/100], Accuracy: 0.4174\n",
      "   \n",
      "Epoch [18/100], Loss: 1.7272\n",
      "Epoch [18/100], Accuracy: 0.4244\n",
      "   \n",
      "Epoch [19/100], Loss: 1.7111\n",
      "Epoch [19/100], Accuracy: 0.4306\n",
      "   \n",
      "Epoch [20/100], Loss: 1.6993\n",
      "Epoch [20/100], Accuracy: 0.4345\n",
      "   \n",
      "Epoch [21/100], Loss: 1.6897\n",
      "Epoch [21/100], Accuracy: 0.4372\n",
      "   \n",
      "Epoch [22/100], Loss: 1.6804\n",
      "Epoch [22/100], Accuracy: 0.4381\n",
      "   \n",
      "Epoch [23/100], Loss: 1.6688\n",
      "Epoch [23/100], Accuracy: 0.4402\n",
      "   \n",
      "Epoch [24/100], Loss: 1.6562\n",
      "Epoch [24/100], Accuracy: 0.4416\n",
      "   \n",
      "Epoch [25/100], Loss: 1.6491\n",
      "Epoch [25/100], Accuracy: 0.4464\n",
      "   \n",
      "Epoch [26/100], Loss: 1.6406\n",
      "Epoch [26/100], Accuracy: 0.4490\n",
      "   \n",
      "Epoch [27/100], Loss: 1.6342\n",
      "Epoch [27/100], Accuracy: 0.4508\n",
      "   \n",
      "Epoch [28/100], Loss: 1.6247\n",
      "Epoch [28/100], Accuracy: 0.4542\n",
      "   \n",
      "Epoch [29/100], Loss: 1.6153\n",
      "Epoch [29/100], Accuracy: 0.4567\n",
      "   \n",
      "Epoch [30/100], Loss: 1.6051\n",
      "Epoch [30/100], Accuracy: 0.4592\n",
      "   \n",
      "Epoch [31/100], Loss: 1.5951\n",
      "Epoch [31/100], Accuracy: 0.4612\n",
      "   \n",
      "Epoch [32/100], Loss: 1.5851\n",
      "Epoch [32/100], Accuracy: 0.4621\n",
      "   \n",
      "Epoch [33/100], Loss: 1.5748\n",
      "Epoch [33/100], Accuracy: 0.4638\n",
      "   \n",
      "Epoch [34/100], Loss: 1.5651\n",
      "Epoch [34/100], Accuracy: 0.4663\n",
      "   \n",
      "Epoch [35/100], Loss: 1.5574\n",
      "Epoch [35/100], Accuracy: 0.4680\n",
      "   \n",
      "Epoch [36/100], Loss: 1.5511\n",
      "Epoch [36/100], Accuracy: 0.4692\n",
      "   \n",
      "Epoch [37/100], Loss: 1.5460\n",
      "Epoch [37/100], Accuracy: 0.4701\n",
      "   \n",
      "Epoch [38/100], Loss: 1.5407\n",
      "Epoch [38/100], Accuracy: 0.4715\n",
      "   \n",
      "Epoch [39/100], Loss: 1.5359\n",
      "Epoch [39/100], Accuracy: 0.4724\n",
      "   \n",
      "Epoch [40/100], Loss: 1.5311\n",
      "Epoch [40/100], Accuracy: 0.4733\n",
      "   \n",
      "Epoch [41/100], Loss: 1.5266\n",
      "Epoch [41/100], Accuracy: 0.4745\n",
      "   \n",
      "Epoch [42/100], Loss: 1.5223\n",
      "Epoch [42/100], Accuracy: 0.4748\n",
      "   \n",
      "Epoch [43/100], Loss: 1.5183\n",
      "Epoch [43/100], Accuracy: 0.4748\n",
      "   \n",
      "Epoch [44/100], Loss: 1.5146\n",
      "Epoch [44/100], Accuracy: 0.4750\n",
      "   \n",
      "Epoch [45/100], Loss: 1.5109\n",
      "Epoch [45/100], Accuracy: 0.4750\n",
      "   \n",
      "Epoch [46/100], Loss: 1.5071\n",
      "Epoch [46/100], Accuracy: 0.4752\n",
      "   \n",
      "Epoch [47/100], Loss: 1.5034\n",
      "Epoch [47/100], Accuracy: 0.4754\n",
      "   \n",
      "Epoch [48/100], Loss: 1.4998\n",
      "Epoch [48/100], Accuracy: 0.4749\n",
      "   \n",
      "Epoch [49/100], Loss: 1.4961\n",
      "Epoch [49/100], Accuracy: 0.4744\n",
      "   \n",
      "Epoch [50/100], Loss: 1.4926\n",
      "Epoch [50/100], Accuracy: 0.4742\n",
      "   \n",
      "Epoch [51/100], Loss: 1.4891\n",
      "Epoch [51/100], Accuracy: 0.4753\n",
      "   \n",
      "Epoch [52/100], Loss: 1.4857\n",
      "Epoch [52/100], Accuracy: 0.4755\n",
      "   \n",
      "Epoch [53/100], Loss: 1.4824\n",
      "Epoch [53/100], Accuracy: 0.4758\n",
      "   \n",
      "Epoch [54/100], Loss: 1.4791\n",
      "Epoch [54/100], Accuracy: 0.4759\n",
      "   \n",
      "Epoch [55/100], Loss: 1.4759\n",
      "Epoch [55/100], Accuracy: 0.4760\n",
      "   \n",
      "Epoch [56/100], Loss: 1.4729\n",
      "Epoch [56/100], Accuracy: 0.4772\n",
      "   \n",
      "Epoch [57/100], Loss: 1.4704\n",
      "Epoch [57/100], Accuracy: 0.4784\n",
      "   \n",
      "Epoch [58/100], Loss: 1.4681\n",
      "Epoch [58/100], Accuracy: 0.4788\n",
      "   \n",
      "Epoch [59/100], Loss: 1.4663\n",
      "Epoch [59/100], Accuracy: 0.4795\n",
      "   \n",
      "Epoch [60/100], Loss: 1.4645\n",
      "Epoch [60/100], Accuracy: 0.4808\n",
      "   \n",
      "Epoch [61/100], Loss: 1.4633\n",
      "Epoch [61/100], Accuracy: 0.4805\n",
      "   \n",
      "Epoch [62/100], Loss: 1.4619\n",
      "Epoch [62/100], Accuracy: 0.4813\n",
      "   \n",
      "Epoch [63/100], Loss: 1.4607\n",
      "Epoch [63/100], Accuracy: 0.4816\n",
      "   \n",
      "Epoch [64/100], Loss: 1.4594\n",
      "Epoch [64/100], Accuracy: 0.4823\n",
      "   \n",
      "Epoch [65/100], Loss: 1.4583\n",
      "Epoch [65/100], Accuracy: 0.4830\n",
      "   \n",
      "Epoch [66/100], Loss: 1.4571\n",
      "Epoch [66/100], Accuracy: 0.4830\n",
      "   \n",
      "Epoch [67/100], Loss: 1.4561\n",
      "Epoch [67/100], Accuracy: 0.4832\n",
      "   \n",
      "Epoch [68/100], Loss: 1.4551\n",
      "Epoch [68/100], Accuracy: 0.4838\n",
      "   \n",
      "Epoch [69/100], Loss: 1.4541\n",
      "Epoch [69/100], Accuracy: 0.4853\n",
      "   \n",
      "Epoch [70/100], Loss: 1.4532\n",
      "Epoch [70/100], Accuracy: 0.4854\n",
      "   \n",
      "Epoch [71/100], Loss: 1.4523\n",
      "Epoch [71/100], Accuracy: 0.4863\n",
      "   \n",
      "Epoch [72/100], Loss: 1.4515\n",
      "Epoch [72/100], Accuracy: 0.4870\n",
      "   \n",
      "Epoch [73/100], Loss: 1.4505\n",
      "Epoch [73/100], Accuracy: 0.4879\n",
      "   \n",
      "Epoch [74/100], Loss: 1.4497\n",
      "Epoch [74/100], Accuracy: 0.4891\n",
      "   \n",
      "Epoch [75/100], Loss: 1.4488\n",
      "Epoch [75/100], Accuracy: 0.4905\n",
      "   \n",
      "Epoch [76/100], Loss: 1.4482\n",
      "Epoch [76/100], Accuracy: 0.4915\n",
      "   \n",
      "Epoch [77/100], Loss: 1.4473\n",
      "Epoch [77/100], Accuracy: 0.4913\n",
      "   \n",
      "Epoch [78/100], Loss: 1.4470\n",
      "Epoch [78/100], Accuracy: 0.4921\n",
      "   \n",
      "Epoch [79/100], Loss: 1.4441\n",
      "Epoch [79/100], Accuracy: 0.4932\n",
      "   \n",
      "Epoch [80/100], Loss: 1.4465\n",
      "Epoch [80/100], Accuracy: 0.4935\n",
      "   \n",
      "Epoch [81/100], Loss: 1.4449\n",
      "Epoch [81/100], Accuracy: 0.4950\n",
      "   \n",
      "Epoch [82/100], Loss: 1.4449\n",
      "Epoch [82/100], Accuracy: 0.4958\n",
      "   \n",
      "Epoch [83/100], Loss: 1.4443\n",
      "Epoch [83/100], Accuracy: 0.4967\n",
      "   \n",
      "Epoch [84/100], Loss: 1.4435\n",
      "Epoch [84/100], Accuracy: 0.4978\n",
      "   \n",
      "Epoch [85/100], Loss: 1.4423\n",
      "Epoch [85/100], Accuracy: 0.4984\n",
      "   \n",
      "Epoch [86/100], Loss: 1.4428\n",
      "Epoch [86/100], Accuracy: 0.4989\n",
      "   \n",
      "Epoch [87/100], Loss: 1.4410\n",
      "Epoch [87/100], Accuracy: 0.4992\n",
      "   \n",
      "Epoch [88/100], Loss: 1.4411\n",
      "Epoch [88/100], Accuracy: 0.5001\n",
      "   \n",
      "Epoch [89/100], Loss: 1.4396\n",
      "Epoch [89/100], Accuracy: 0.5001\n",
      "   \n",
      "Epoch [90/100], Loss: 1.4399\n",
      "Epoch [90/100], Accuracy: 0.5004\n",
      "   \n",
      "Epoch [91/100], Loss: 1.4363\n",
      "Epoch [91/100], Accuracy: 0.5001\n",
      "   \n",
      "Epoch [92/100], Loss: 1.4388\n",
      "Epoch [92/100], Accuracy: 0.5006\n",
      "   \n",
      "Epoch [93/100], Loss: 1.4361\n",
      "Epoch [93/100], Accuracy: 0.5007\n",
      "   \n",
      "Epoch [94/100], Loss: 1.4364\n",
      "Epoch [94/100], Accuracy: 0.5017\n",
      "   \n",
      "Epoch [95/100], Loss: 1.4342\n",
      "Epoch [95/100], Accuracy: 0.5012\n",
      "   \n",
      "Epoch [96/100], Loss: 1.4346\n",
      "Epoch [96/100], Accuracy: 0.5020\n",
      "   \n",
      "Epoch [97/100], Loss: 1.4320\n",
      "Epoch [97/100], Accuracy: 0.5022\n",
      "   \n",
      "Epoch [98/100], Loss: 1.4332\n",
      "Epoch [98/100], Accuracy: 0.5025\n",
      "   \n",
      "Epoch [99/100], Loss: 1.4313\n",
      "Epoch [99/100], Accuracy: 0.5036\n",
      "   \n",
      "Epoch [100/100], Loss: 1.4311\n",
      "Epoch [100/100], Accuracy: 0.5032\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "\n",
    "lstm = LSTM(seq_length = len(X[0]), \n",
    "            hidden_size = hidden_size, \n",
    "            num_layers = num_layers, \n",
    "            vocab_size = vocab_size)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(params = lstm.parameters(), lr = .001)\n",
    "lstm.train(torch_data, 100, loss_function, optimizer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a Proofs DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ls_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
