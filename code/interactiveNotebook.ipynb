{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proving Theorems In Propositional Logic with LSTM-Based Text Generators\n",
    "#### Author: Omar Afifi\n",
    "\n",
    "Consider a number of propositional (i.e. variable-free) sentences (premsises). For example: \n",
    "\n",
    "1. p→q \n",
    "2. o\n",
    "3. pv¬o\n",
    "\n",
    "A propositional proof from the premises to a conclusion (another sentence) is a sequence of variable-free statements that follow from the premises by logical deduction rules (e.g. modus ponens, modus tollens, modus tollendo ponens, dysjuntive elimination, etc ... )\n",
    "\n",
    "For example, a proof of the propositional sentence (q) from the preceding premises is as follows: \n",
    "\n",
    "4. ¬¬o (from 2, double negation)\n",
    "5. p (from 3 and 4, dysjuntive elimination )\n",
    "5. q (from 1 and 5, modus ponens)\n",
    "\n",
    "QED.\n",
    "\n",
    "\n",
    "This notebook explores the utility of using LSTM text-generators to generate a propositional proof given a collection of propositional sentences. Our hope is that it can be helpful as a stepping stone to making progress in the arena of stochastic theorem provers. \n",
    "\n",
    "Credits: Hugging Face User ergotts for building this dataset: https://huggingface.co/datasets/ergotts/propositional-logic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data and preparing the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import process_data\n",
    "\n",
    "#load the data from hugging face mode = 'w' means that we are tokenizing words rather than characters or sentences. \n",
    "proofs_dataset = process_data.LoadLogicData(mode = 'w') \n",
    "\n",
    "#format the proofs: essentially just mapping words to integers and then creating n-gram sequences\n",
    "word_to_int, int_to_word, sequenced_proofs = process_data.generate_sequences(proofs_dataset)\n",
    "\n",
    "#split data into input and label by setting label equal to next word.\n",
    "#sequence length is the length of eqch sequence, this allows us to pack them during training. \n",
    "X, sequence_lengths,y = process_data.makeXy(sequenced_proofs)\n",
    "\n",
    "X_train, X_val, train_lengths, val_lengths, y_train, y_val = process_data.makeSplit(X,sequence_lengths, y, .80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Data Compatible With Torch API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.utils.data  as data\n",
    "\n",
    "#train data\n",
    "X_train = t.tensor(X_train, dtype = t.int64)\n",
    "train_lengths =  t.tensor(train_lengths, dtype = t.int64)\n",
    "y_train = t.tensor(y_train, dtype = t.int64)\n",
    "\n",
    "train_loader = data.DataLoader(data.TensorDataset(X_train,train_lengths, y_train),\n",
    "                            batch_size = 100) #model expects training data to be batched a dataLoader\n",
    "\n",
    "#validation data\n",
    "X_val = t.tensor(X_val, dtype = t.int64)\n",
    "val_lengths =  t.tensor(val_lengths, dtype = t.int64)\n",
    "y_val = t.tensor(y_val, dtype = t.int64)\n",
    "\n",
    "validation_data = (X_val, val_lengths, y_val) # this model will expect validation data to be a tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Model and Training\n",
    "Note: This model does not suport GPU processing. It only takes ~10 minutes to run on a M1 CPU. \n",
    "\n",
    "If you want to make it GPU proccessable, then you need to edit model.py and write the tensors to the GPU as appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set Your params as desired here\n",
    "import model\n",
    "import torch\n",
    "\n",
    "vocab_size = len(word_to_int)\n",
    "hidden_size = 120\n",
    "num_layers = 2\n",
    "epochs = 24\n",
    "loss_function = torch.nn.CrossEntropyLoss(reduction = \"mean\")\n",
    "seq_length = len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lstm = model.LSTM(seq_length = seq_length, \n",
    "            hidden_size = hidden_size, \n",
    "            num_layers = num_layers, \n",
    "            vocab_size = vocab_size)\n",
    "\n",
    "optimizer = torch.optim.Adam(params = lstm.parameters(), lr = .01)\n",
    "training_accuracies, training_losses, validation_accuracies, validation_losses = lstm.Train(train_loader, \n",
    "                                                                                            epochs, \n",
    "                                                                                            loss_function, \n",
    "                                                                                            optimizer, \n",
    "                                                                                            validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import evaluate\n",
    "\n",
    "fig, axs = evaluate.plotResults(epochs, training_accuracies, \n",
    "                                validation_accuracies, \n",
    "                                training_losses, validation_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asking the Model to Generate a Proof From Premises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads some unseen premises from hugging face\n",
    "\n",
    "test_data = process_data.load_test_data()\n",
    "test_sequence_lengths, test_data = process_data.process_test_data(test_data, word_to_int)\n",
    "test_data = process_data.pad(test_data, 0, seq_length)\n",
    "\n",
    "#If you want to save the proofs to your folder, you can specify a path and set save = true in generate_proofs\n",
    "#generated proofs returns a list of coleted proofs, it will also print the first value as a sample\n",
    "\n",
    "path = \"\"\n",
    "generated_proofs = evaluate.generate_proofs(lstm, test_data, test_sequence_lengths, int_to_word, save = True, path = path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_proofs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See How More Data and Larger Models Improve Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell will take ~ 30-40 minutes to run. \n",
    "#%run growth.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ls_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
